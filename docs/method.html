<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <title>Method</title>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" type="text/css" href="styling.css" />

    </head>
    <body>
        <div class="wrap">
            <div id="sidebar">
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="corpus.html">Corpus</a></li>
                    <li><a href="method.html">Method</a></li>
                    <li><a href="analysis.html">Analysis</a></li>
                    <li><a href="nextStep.html">Next Steps</a></li>
                </ul>
            </div>
            <div class = "content">
                <div class="headCont">                    
                    <h1 class="header">Methods Used</h1>
                </div>
                <div class="container">
                    <a href="https://github.com/Cal16king/Finance-Sentiment-Analysis/blob/main/Python/web_scraper.py"><h2>Web Scraper</h2></a>
                    <p>A Python script was developed to scrape headline articles from CNBC every hour, from 9 a.m. to 4 p.m., on weekdays. These specific times were chosen to capture headlines approximately every hour while the stock market is open. The script utilized the Python library BeautifulSoup for web scraping, and a DigitalOcean server with crontab was employed to automate the process. The script generated a JSON file containing all the collected data. The data was collected from April 1 to April 18.
                    </p>
                </div>
                <div class="container">
                    <h2>Data Cleaning
                    </h2>
                    <p>The data underwent manual cleaning to ensure quality. This process involved removing duplicate articles and adding corresponding dates to the records. Human sentiment was appended to each article to facilitate future analysis. Additionally, the JSON data was converted to XML, providing more format options for flexible analysis.
                    </p>
                </div>
                <div class="container">
                    <h2>Data Cleaning</h2>
                    <p>Our data was cleaned manually. Thankfully, our data collection had been optimized by our web scraper, so there were only a few tokens that need removed along with making the data look more formal. Due to our web scraper outputting a JSON format, we used a JSON to xml converter for more convenient use of processing.</p>
                </div>
                <div class="container">
                    <h2>Text Processing</h2>
                    <p>Our processing of our corpus was completed with the help of XSLT. We ran the program to have us seperate the corpus into its respective articles.</p>
                    <p>Inflation</p>
                    <p>Year</p>
                    <p>Market</p>
                    <p>Rates</p>
                </div>
                <div class="container">
                    <h2>Data Aggregation</h2>
                    <p>To give us a hint about what key words to start with, we used Voyant to give us the most frequently used terms. The words returned to us were: </p>
                    <p>-Inflation</p>
                    <p>-Year</p>
                    <p>-Market</p>
                    <p>-Rates</p>
                    <p></p>
                    <p></p>
                    <p>Given a good start on where to look, we used Xquery for any other key words to get their word count. We also further catagorized them by seperating the word count for each article to see which ones had unique words used more often than others.</p>
                </div>
            </div>
        </div>
    </body>
</html>
